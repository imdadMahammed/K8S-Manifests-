                                                                     EKS 
#                                                                  -----------
# We are creating Managed K8s Cluster. (https://github.com/bLakshmankumar/HPA_Deployment/blob/main/EKS_SetUp)
# Connect to that server which we created before, then create some deployment file for deploy one sample application.
# apiVersion: apps/v1
kind: Deployment
metadata:
  name: wordpress
  labels:
    app: wordpress
spec:
  selector:
    matchLabels:
      app: wordpress
  template:
    metadata:
	  name: wordpress_conatainer
      labels:
        app: wordpress
    spec:
      containers:
	  - name: wordpress
        image: wordpress:latest
		ports:
		- containerPort: ****
		resources:
		  requests:
		     cpu: 200m
			 memory: 1Gi
		 limits:
		    cpu: 500m
			memory: 2Gi
---
apiVersion: v1
kind: Service
metadata:  
  name: wordpress-service
spec:
  type: LoadBalancer
  ports:  
  -  port: 80
    targetPort: 8080
  selector:    
    app: wordpress.


# > Before deploy this app let check do we have any pods are running in kube-system
# > kubectl get all -n kube-system
# > Here/Under this kube system we have some applications, we can see some aws related pods.
# > The aws has Cloud Controller, The cloud controller act as a bridge between the k8s cluster and the Cloud.
# > It'll have a access to the aws to create other aws services.

# > Now we create an app deployment with loadbalancer. So now i deploy this application with loadbalancer service.
# > Now we get an one LoadBalancer under Ec2 dashboard. let's apply that File.yml
# > kubectl apply -f yml name
# > kubectl get pods -o wide
# > kubectl get svc -: now we can see the service type of loadbalancer with DNS in External IP.
# > Now Go to aws console under loadbalancers we can see the load balancer with this Dns name and port and targetport.
# > if you delete the node also it'll recreate the new node and adjest the pods.
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

# How request is reach to conatiner -:
----------------------------------------------
# Client > DNS (it'll identifies the ip address of the domain) > LoadBalancer > k8s Nodes with that port > Service > POD.
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

# How to do a Cluster Autoscaler -:
--------------------------------------------
# > For that we need to deploly the Cluster Autoscaler application.
# > The Kubernetes Cluster Autoscaler automatically adjusts the number of nodes in your cluster when pods fail or are rescheduled onto other nodes.
# > The Cluster Autoscaler is typically installed as a Deployment in your cluster. It uses leader election to ensure high availability, but scaling is done by only one replica at a time.
# > The Cluster autoscaler act as a bridge between K8s Cluster and Cloud. So Cluster autoscaler will talk to the api server.
# > It'll identify any pod is in pending state. It'll talk to the api server and it'll identifies is there any pods that are not scheduled bcs of no resources are not available.
# > If it identifies that pods are in pending state, it'll talk to the aws and it'll uupdate the autoscaling group desired numbers. then autoscaling will increase the numbers.
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Steps to create a cluster Autoscaler -:
------------------------------------------------
# 1) Create Aws IAM Role with autoscaling permisiions
# 2) Attach Policy to IAM Role which is used in EKS Node Group 
# 3 ) Deploy ClusterAutoscaler using below yml(make sure we update - --node-group-auto-discovery values with our cluster name under cluster-autoscaler deployment Commands.)

# How to Deploy Cluster Autoscaler -:
------------------------------------------------
# Create a ClusterAutoscaler.yml with  k8s.gcr.io/autoscaling/cluster-autoscaler:v1.22.2 this image ( This image is create and managed by k8s itself)
# We Just deploy that clusterAutoscaler image as a k8s pod.
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
# ---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
  name: cluster-autoscaler
  namespace: kube-system

# ---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cluster-autoscaler
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
rules:
  - apiGroups: [""]
    resources: ["events", "endpoints"]
    verbs: ["create", "patch"]
  - apiGroups: [""]
    resources: ["pods/eviction"]
    verbs: ["create"]
  - apiGroups: [""]
    resources: ["pods/status"]
    verbs: ["update"]
  - apiGroups: [""]
    resources: ["endpoints"]
    resourceNames: ["cluster-autoscaler"]
    verbs: ["get", "update"]
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["watch", "list", "get", "update"]
  - apiGroups: [""]
    resources:
      - "namespaces"
      - "pods"
      - "services"
      - "replicationcontrollers"
      - "persistentvolumeclaims"
      - "persistentvolumes"
    verbs: ["watch", "list", "get"]
  - apiGroups: ["extensions"]
    resources: ["replicasets", "daemonsets"]
    verbs: ["watch", "list", "get"]
  - apiGroups: ["policy"]
    resources: ["poddisruptionbudgets"]
    verbs: ["watch", "list"]
  - apiGroups: ["apps"]
    resources: ["statefulsets", "replicasets", "daemonsets"]
    verbs: ["watch", "list", "get"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses", "csinodes", "csidrivers", "csistoragecapacities"]
    verbs: ["watch", "list", "get"]
  - apiGroups: ["batch", "extensions"]
    resources: ["jobs"]
    verbs: ["get", "list", "watch", "patch"]
  - apiGroups: ["coordination.k8s.io"]
    resources: ["leases"]
    verbs: ["create"]
  - apiGroups: ["coordination.k8s.io"]
    resourceNames: ["cluster-autoscaler"]
    resources: ["leases"]
    verbs: ["get", "update"]

# We are creating a service account with cluster-autoscaler name and we are creating a role with specific permissions for nodes and pods.
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

# > why we give the permisions bcs this service account we are using with the pods(the app servers) the pods can talk to the api server and 
# > Programetically get the status of the  pods, nodes. 
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
rules:
  - apiGroups: [""]
    resources: ["configmaps"]
    verbs: ["create","list","watch"]
  - apiGroups: [""]
    resources: ["configmaps"]
    resourceNames: ["cluster-autoscaler-status", "cluster-autoscaler-priority-expander"]
    verbs: ["delete", "get", "update", "watch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cluster-autoscaler
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-autoscaler
subjects:
  - kind: ServiceAccount
    name: cluster-autoscaler
    namespace: kube-system

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cluster-autoscaler
subjects:
  - kind: ServiceAccount
    name: cluster-autoscaler
    namespace: kube-system

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    app: cluster-autoscaler
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cluster-autoscaler
  template:
    metadata:
      labels:
        app: cluster-autoscaler
      annotations:
        prometheus.io/scrape: 'true'
        prometheus.io/port: '8085'
    spec:
      priorityClassName: system-cluster-critical
      securityContext:
        runAsNonRoot: true
        runAsUser: 65534
        fsGroup: 65534
      serviceAccountName: cluster-autoscaler
      containers:
        - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.22.2
          name: cluster-autoscaler
          resources:
            limits:
              cpu: 100m
              memory: 600Mi
            requests:
              cpu: 100m
              memory: 600Mi
          command:
            - ./cluster-autoscaler
            - --v=4
            - --stderrthreshold=info
            - --cloud-provider=aws
            - --skip-nodes-with-local-storage=false
            - --expander=least-waste
            - --nodes=1:10:k8s-worker-asg-1
            - --nodes=1:3:k8s-worker-asg-2
# Update Your Cluster name here.

          volumeMounts:
            - name: ssl-certs
              mountPath: /etc/ssl/certs/ca-certificates.crt #/etc/ssl/certs/ca-bundle.crt for Amazon Linux Worker Nodes
              readOnly: true
          imagePullPolicy: "Always"
      volumes:
        - name: ssl-certs
          hostPath:
            path: "/etc/ssl/certs/ca-bundle.crt"
			
			
# Let's apply the file.yml 
# We deploy the cluster Autoscaler so we is the expectation of autoscaler it should be able to scaleup the cluster. but it's not scaleup/we can't see the new node
# let's check the cluster pod logs -: kubectl logs clusterpod name  -n kube-system. 
# Bcs the node doesn't have an aws permisiions. so it can't talk to the autoscaling group and it can't be update the desired num.
# For that we need to create and attach IAM Role
# We already attach some rules to our worker nodes while creating a worker group. 
# But we need to create another autoscaling role to the nodes.
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
# 1) Create IAM policy with below actions.(Json).
# {
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "autoscaling:DescribeAutoScalingGroups",
        "autoscaling:DescribeAutoScalingInstances",
        "autoscaling:DescribeLaunchConfigurations",
        "autoscaling:DescribeTags",
        "ec2:DescribeInstanceTypes",
        "ec2:DescribeLaunchTemplateVersions"
      ],
      "Resource": ["*"]
    },
    {
      "Effect": "Allow",
      "Action": [
        "autoscaling:SetDesiredCapacity",
        "autoscaling:TerminateInstanceInAutoScalingGroup",
        "ec2:DescribeImages",
        "ec2:GetInstanceTypesFromInstanceRequirements",
        "eks:DescribeNodegroup"
      ],
      "Resource": ["*"]
    }
  ]
}

# 2) Attach the Policy to IAM Role which is used in EKS Node Group.( Go to EKS click on Cluster name > under details we can see the Node IAM Role Click on it > Click on Attach policy and add.
 
 3) No check the nodes and pods.
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
# Now it has all the permisions so now it can talk to the aws and we can see the scale up in logs
# kubectl logs cluster-autoscalerpod name. -: now we can see it'll change the desired number.
# kubectl get nodes -: now we can see the new nodes and the sheduler can able to shedule the pods.
# If i scale the pods also the new node will be come up and containers will be create based on the max state.
